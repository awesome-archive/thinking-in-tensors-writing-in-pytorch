{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking in tensors in PyTorch\n",
    "\n",
    "Hands-on training  by [Piotr Migdał](https://p.migdal.pl) (2019). \n",
    "\n",
    "Version for [AI & NLP Workshop Day](https://nlpday.pl/), 31 May 2019, Warsaw, Poland: **Understanding LSTM and GRU networks in PyTorch**.\n",
    "\n",
    "> Long short-term memory (LSTM) and gated recurrent unit (GRU) network are popular network architectures for text processing. During this workshop (held in PyTorch) we will work with them in a low-level way, getting access to memory cells and intermediate states. Targeted at people using LSTMs/GRUs as black boxes OR have a background in other network architectures and would like to understand natural language processing with deep learning.\n",
    "\n",
    "\n",
    "## NLP & AI: 1. RNN architecture overview\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/stared/thinking-in-tensors-writing-in-pytorch/blob/master/extra/1%20RNN%20architecture%20overview.ipynb)\n",
    "\n",
    "We use recurrent networks. For wonderful introductions:\n",
    "\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah\n",
    "* [Exploring LSTMs](http://blog.echen.me/2017/05/30/exploring-lstms/) by Edwin Chen\t\n",
    "\n",
    "See also:\n",
    "\n",
    "* [Simple diagrams of convoluted neural networks](https://medium.com/inbrowserai/simple-diagrams-of-convoluted-neural-networks-39c097d2925b) by Piotr Migdał\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy\n",
    "* [Repository to track the progress in Natural Language Processing](https://github.com/sebastianruder/NLP-progress) by Sebastian Ruder\n",
    "* [Memorization in RNNs](https://distill.pub/2019/memorization-in-rnns/)\n",
    "\n",
    "And a few technical remarks:\n",
    "\n",
    "* [Inconsistent dimension ordering for 1D networks - NCL vs NLC vs LNC](https://discuss.pytorch.org/t/inconsistent-dimension-ordering-for-1d-networks-ncl-vs-nlc-vs-lnc/14807)\n",
    "* [Contiguous() and permute()](https://discuss.pytorch.org/t/contiguous-and-permute/20673)\n",
    "\n",
    "How to think about tensors:\n",
    "\n",
    "* [Named tensors](http://nlp.seas.harvard.edu/NamedTensor) and [Named tensors (part 2)](http://nlp.seas.harvard.edu/NamedTensor2) by Alexander Rush \n",
    "* [Matrices as Tensor Network Diagrams](https://www.math3ma.com/blog/matrices-as-tensor-network-diagrams) by Tai-Danae Bradley\n",
    "* There are Named Tensors with [PyTorch 1.3.0 release](https://github.com/pytorch/pytorch/releases/tag/v1.3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a few examples in... Keras\n",
    "\n",
    "* [Keras or PyTorch as your first deep learning framework](https://deepsense.ai/keras-or-pytorch/) by Rafał Jakubanis and Piotr Migdał\n",
    "\n",
    "At least for the first approach, Keras may over an easier start:\n",
    "\n",
    "* [Recurrent Layers - Keras](https://keras.io/layers/recurrent/)\n",
    "* [François Chollet, \"Deep Learning with Python\"](https://www.manning.com/books/deep-learning-with-python), Chapter 6. Deep learning for text and sequences\n",
    "\n",
    "Here we use [keras-sequential-ascii](https://github.com/stared/keras-sequential-ascii) package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# if you need to install that\n",
    "!pip install -q keras_sequential_ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, SimpleRNN, Dense, Dropout, LSTM, GRU, Bidirectional\n",
    "\n",
    "from keras_sequential_ascii import sequential_model_to_ascii_printout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Recurrent Neural Networks\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "\n",
    "from [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####     10   26\n",
      "           SimpleRNN   ????? -------------------      1888    92.0%\n",
      "                tanh   #####          32\n",
      "               Dense   XXXXX -------------------       165     8.0%\n",
      "             softmax   #####           5\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    SimpleRNN(32, return_sequences=False, input_shape=(10, 26)),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "sequential_model_to_ascii_printout(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long short-term memory (LSTM)\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "from [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####     10   26\n",
      "                LSTM   LLLLL -------------------      7552    97.9%\n",
      "                tanh   #####          32\n",
      "               Dense   XXXXX -------------------       165     2.1%\n",
      "             softmax   #####           5\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(10, 26)),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "sequential_model_to_ascii_printout(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Bidirectional(LSTM(32, input_shape=(10, 26), return_sequences=True)),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# sequential_model_to_ascii_printout(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####     10   26\n",
      "                LSTM   LLLLL -------------------      7552    47.1%\n",
      "                tanh   #####     10   32\n",
      "                LSTM   LLLLL -------------------      8320    51.9%\n",
      "                tanh   #####          32\n",
      "               Dense   XXXXX -------------------       165     1.0%\n",
      "             softmax   #####           5\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(10, 26), return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "sequential_model_to_ascii_printout(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU)\n",
    "\n",
    "To some extent- \n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)\n",
    "\n",
    "from [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####     10   26\n",
      "                 GRU   LLLLL -------------------      5664    46.9%\n",
      "                tanh   #####     10   32\n",
      "                 GRU   LLLLL -------------------      6240    51.7%\n",
      "                tanh   #####          32\n",
      "               Dense   XXXXX -------------------       165     1.4%\n",
      "             softmax   #####           5\n"
     ]
    }
   ],
   "source": [
    "# GRU is a drop-on replacement\n",
    "model = Sequential([\n",
    "    GRU(32, input_shape=(10, 26), return_sequences=True),\n",
    "    GRU(32),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "sequential_model_to_ascii_printout(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in Keras is is simpler, why bother?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
