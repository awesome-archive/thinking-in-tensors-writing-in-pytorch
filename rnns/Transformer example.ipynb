{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking in tensors in PyTorch\n",
    "\n",
    "Hands-on training  by [Piotr MigdaÅ‚](https://p.migdal.pl) (2019). \n",
    "\n",
    "\n",
    "## Transformer models\n",
    "\n",
    "\n",
    "* [GPT-2 - better language models and their implications](https://openai.com/blog/better-language-models/) by Open AI\n",
    "\n",
    "\n",
    "PROMPT: \n",
    "\n",
    "> **Cities & Lights**\n",
    "> \n",
    "> When you enter the city of Singapore during the night, you see lights: colorful and ubiquitous. Lights on every building, on every fountain, and in every park.\n",
    "\n",
    "GENERATED:\n",
    "\n",
    ">  Lights shining in a city in which the majority of people are now using mobile phones. Singapore has a bright future as a technology hub, and it 's not too late to make it happen.\n",
    "\n",
    "Inspired by [Invisible Cities by Italo Calvino](https://en.wikipedia.org/wiki/Invisible_Cities).\n",
    "\n",
    "### Interactive demos\n",
    "\n",
    "* [Write With Transformer by Hugging Face](https://transformer.huggingface.co/)\n",
    "    * [GPT-2 large](https://transformer.huggingface.co/doc/gpt2-large)\n",
    "* [Gwern's AI-generated poetry](https://slatestarcodex.com/2019/03/14/gwerns-ai-generated-poetry/) and [GPT-2 Neural Network Poetry](https://www.gwern.net/GPT-2)\n",
    "* [AI Dungeon](https://www.aidungeon.io/) - a text-based adventure game powered by GPT-2\n",
    "\n",
    "### This example\n",
    "\n",
    "https://github.com/huggingface/pytorch-transformers\n",
    "\n",
    "https://huggingface.co/pytorch-transformers/index.html\n",
    "\n",
    "Heavily based on https://github.com/huggingface/pytorch-transformers/blob/master/examples/run_generation.py\n",
    "\n",
    "Note: models are BIG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_transformers import GPT2Config\n",
    "from pytorch_transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config,)), ())\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer)\n",
    "}\n",
    "\n",
    "print(ALL_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(length):\n",
    "\n",
    "            inputs = {'input_ids': generated}\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'gpt2' #@param [\"gpt2\"]\n",
    "model_name_or_path = 'gpt2-medium'  #@param [\"gpt2\", \"gpt2-medium\", \"gpt2-large\"]\n",
    "device = 'cuda'  #@param [\"cuda\", \"cpu\"]\n",
    "# device auto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of args\n",
    "model_class, tokenizer_class = MODEL_CLASSES[model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line downloads things\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this even more\n",
    "# and loading itself takes ~20 sec\n",
    "model = model_class.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_length = 64  #@param {type:\"integer\"}\n",
    "temperature = 1.  #@param {type:\"slider\", min:0.1, max:5.0, step:0.1}\n",
    "top_k = 50  #@param {type:\"integer\"}\n",
    "top_p = 0.  #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "text_prompt = 'Before going into the wilderedness, make sure that' #@param {type:\"string\"}\n",
    "\n",
    "context_tokens = tokenizer.encode(text_prompt)\n",
    "out = sample_sequence(\n",
    "    model=model,\n",
    "    context=context_tokens,\n",
    "    length=generate_length,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k\n",
    "    top_p=top_p,\n",
    "    device=device\n",
    ")\n",
    "out = out[0, len(context_tokens):].tolist()\n",
    "text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(raw_text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
